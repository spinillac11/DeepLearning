{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION to PyTorch\n",
    "\n",
    "This notebook provides a simple introduction to pytorch, assuming you already know **Python, NumPy** and how to use **Jupyter/Colab** notebooks. \n",
    "\n",
    "**PyTorch** is a Python library designed for **deep learning**. All calculations rely on one type of a fundamental data structure: the **tensor**. A tensor is a generalization of vectors (1 dimension) and matrices (2 dimensions) to higher dimensions. For instance:\n",
    "\n",
    "- A set of matrices with the same dimensions forms a 3-dimensinal tensor. \n",
    "- A colored image consists of 3 matrices (one per chanel: red, blue green), making it a tensor. \n",
    "- A set of images with the same resolution forms a tensor with 4 dimensions\n",
    "\n",
    "In deep learning everything is represented as tensors: the input data, model parameters, loss functions, gradients, etc. All computations involve operations on tensors, which makes PyTorch very powerful and flexible library for machine learning. \n",
    "\n",
    "\n",
    "Machine learning basically relies on three key components: \n",
    "\n",
    "- **The model:** in this course, the model is a Neural Network, which is represented in pytorch as a **Module**.\n",
    "- **The loss function:** the model is trained to minimize a loss function, which measures how well it performs.\n",
    "- **The optimizer:** minimization of the loss function is achieved with gradient descent (or one of its variants) using an **optimizer**.\n",
    "\n",
    "Moreover, to perform gradient descent, we need to compute the gradient of the loss with respect to the model’s parameters. This is the goal of the so-called **auto-differentiation** which provides efficient computation of these gradients. \n",
    "\n",
    "In this notbook we will introduce these 3 components, along with PyTorch's basic data structure (the tensors) and the concept of auto-differentiation. \n",
    "\n",
    "\n",
    "To start with pytorch, here are some external websites: \n",
    "- http://pytorch.org/tutorials/ : official tutorials\n",
    "- http://pytorch.org/docs/master/ : official documentation\n",
    "\n",
    "Before you start, check the version of pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "print(th.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should have a version of at least 1.0.0.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need help ? \n",
    "help(th.arange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or \n",
    "?th.arange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors \n",
    "\n",
    "In NumPy or MATLAB, you have several methods to create arrays: `zeros`, `ones`, `arange`, `rand`, `randn`... PyTorch uses analogous functions for creating and manipulating tensors, which you will learn how to use. By using these functions, you can easily create tensors and perform various operations on them. For example, PyTorch offers `torch.zeros`, `torch.ones`, `torch.arange`, `torch.rand`, and `torch.randn` to help you initialize tensors in a manner similar to NumPy or MATLAB.\n",
    "\n",
    "\n",
    "\n",
    "## 1) Operation and access\n",
    "\n",
    "### **Do the following:**\n",
    "\n",
    "- Create a Tensor:\n",
    "\n",
    "        Build a tensor with dimensions 3×4 that is filled with the numbers from 1 to 12.\n",
    "\n",
    "- Extract Rows and Columns:\n",
    "\n",
    "        Extract the first row and the last row of the tensor you built.\n",
    "\n",
    "        Similarly, extract the first column and the last column.\n",
    "\n",
    "- Matrix Operations with Random Initialization:\n",
    "\n",
    "        Create matrix A with dimensions 2×3.\n",
    "\n",
    "        Create matrix B with dimensions 2×1.\n",
    "\n",
    "        Create matrix C with dimensions 1×4.\n",
    "\n",
    "        Concatenate matrices A and B (think about the compatibility of the dimensions!!).\n",
    "\n",
    "        Add the resulting matrix to C.\n",
    "\n",
    "- Row Selection:\n",
    "\n",
    "        Create matrix A with dimensions 5×4.\n",
    "\n",
    "        Create matrix B with dimensions 3×4, such that B is composed of rows from A in this order: the second row, the first row, and the fourth row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following code and how  x2 is built from x. Look at the dimensions of the created tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(5, 4)\n",
    "print(x)\n",
    "\n",
    "x2= th.stack((x,x) , dim=0)\n",
    "print (x2[0]) \n",
    "print (x2.size()) \n",
    "\n",
    "x2= th.stack((x,x) , dim=1)\n",
    "print (x2[0]) \n",
    "print (x2.size()) \n",
    "\n",
    "x2= th.stack((x,x) , dim=2)\n",
    "print (x2[0]) \n",
    "print (x2.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Reshape and squeeze\n",
    "\n",
    "The method `view()` is similar to `reshape` from NumPy. This is **important** since with neural nets, you will often need to manipulate dimensions. \n",
    "\n",
    "### **Do the following:**\n",
    "\n",
    "- Build a tensor with dimensions (2, 3, 4).\n",
    "\n",
    "- Reshape this tensor into two matrices:\n",
    "    \n",
    "        One with dimensions (3, 8).\n",
    "        Another with dimensions (2, 12).\n",
    "\n",
    "- What does  `view(2,-1)`  do ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have a Tensor A of dimensions (3,2,1) that you initialized as you want.\n",
    "\n",
    "### **Do the following:**\n",
    "\n",
    "- Read the documentation of the method `squeeze`.\n",
    "\n",
    "- Try it on A.\n",
    "\n",
    "- Try the reverse with `unsqueeze`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Automatic differentiation  (auto-grad)\n",
    "\n",
    "`torch.autograd` provides classes and functions implementing automatic differentiation. \n",
    "When a tensor is created with `requires_grad=True`, the object will be able to store information about\n",
    "\n",
    "- *who has created it,*\n",
    "- *the gradient, if in the future we decide to compute the gradients of some scalar function.*\n",
    "\n",
    "In the following example, we build a computational graph. The \"end\" of this graph must be a scalar for automatic differentiation. Look at the following code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(1, 1, requires_grad=True)\n",
    "print(\"x:\",x)\n",
    "print(\"x.grad:\",x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor `x` has an attribute `grad`, which is initially set to `None`. When you perform operations on `x` using PyTorch functions, a computational graph is being built automatically! This graph keeps track of all operations performed on x and any subsequent tensors derived from it. \n",
    "\n",
    "The function `backward()` can be called on a tensor that contains a scalar. This call computes the gradients (partial derivatives) of the this scalar value with respect to all the tensors that were involved in its computation. Look at the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*x+1 \n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question:**\n",
    "*Do you agree with the result ?* (check yourself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph can of course be deeper. For instance, we can introduce a new variable `w`, that is a tensor as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.randn(1,1, requires_grad=True)\n",
    "x = th.randn(1, 1, requires_grad=True)\n",
    "\n",
    "print(\"w=\",w.item(),\" and x=\",x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = w*x\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question:**\n",
    "\n",
    "*What do you think about the result ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noticing that the gradient is a tensor operation on a scalar value: **we compute the partial derivative of a scalar quantity w.r.t a tensor**. The variable on which we run the `backward()` method must be a scalar. \n",
    "\n",
    "\n",
    "### **Question:**\n",
    "\n",
    "Consider the following code: can you explain the results ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(2,2,requires_grad=True)        # x is a square matrix\n",
    "print(x)\n",
    "out = 0.5*x.pow(2).sum() # out is a new variable (scalar)\n",
    "out.backward()           # back propagation in the graph\n",
    "print(\"g:\",x.grad)       # the gradient of out with respect to x \n",
    "print(\"x:\",x)            # A simple check. Is it what expected ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Question:**\n",
    "\n",
    "And can you explain this example  ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(2,2,requires_grad=True)        # x is a square matrix\n",
    "print(x)\n",
    "w = th.ones(1,2,requires_grad=True)\n",
    "print(w)\n",
    "out = 0.5*(w@x).pow(2).sum() # out is a new variable (scalar)\n",
    "out.backward()           # back propagation in the graph\n",
    "print(\"x:\",x)            \n",
    "print(\"x.grad:\",x.grad)  # the gradient of out with respect to x \n",
    "print(\"w:\",w)            \n",
    "print(\"w.grad:\",w.grad)  # the gradient of out with respect to x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Linear regression with gradient descent\n",
    "\n",
    "In this section we focus on linear regression, using synthetic data. \n",
    "\n",
    "The data are generated with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(123) # to ensure reproducibility\n",
    "var= 1 # \n",
    "X = th.arange(8) + th.randn(8)/var\n",
    "Y = 2*(th.arange(8) + th.randn(8)/var) + 0.5 \n",
    "\n",
    "_ = plt.scatter(X,Y)\n",
    "\n",
    "_ = plt.xlabel('x')\n",
    "_ = plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to approximate this simple data set with a function $f$ such that \n",
    "$$\n",
    "y_i \\approx f(x_i).\n",
    "$$ \n",
    "\n",
    "To find the right set of parameters that define $f$, we want to minimize the mean square error:\n",
    "$$L = \\frac{1}{N} \\sum_i^N (f(x_i) - y_i)^2.$$\n",
    "\n",
    "Our first assumption is that $f$ is a linear function: \n",
    "$$ f(x) = w\\ x+w_0.$$.\n",
    "\n",
    "The optimization procedure (here: gradient descent) starts with the random initialization of the parameters $(w,w_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.randn(1,requires_grad=True)\n",
    "w0= th.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you plot a linear function $f(x)$ using these randomly chosen parameters $w, w_0$, together with your training points (evaluate the cell belwo), you will see that $f(x)$ does not describe the data very well (or you are very lucky with the random numbers! try again if you are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = th.linspace(0,8,2) # need 2 points for a line\n",
    "plt.plot(xs,(w*xs+w0).detach(),'r', label = 'f(x) using random params')\n",
    "plt.scatter(X,Y, label = 'data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Do the following:** \n",
    "\n",
    "Quantify the poor quality of this random initialization by computing the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Do the following:** \n",
    "\n",
    "Use the `backward` function to get the gradient of the loss with respect to the parameters. \n",
    "- Print the gradients.\n",
    "- Propose an update of the parameters \n",
    "- Verify if it improves the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Optimizer and the loss function\n",
    "\n",
    "The optimizer \"takes care\" of model parameters update during training. In PyThorch, the base class for all optimizers is `Optimizer`, which is a part of the module `torch.optim`. As you discussed in TD, one commonly used optmizer is Stochastic Gradient Descent `SGD`. Look at the details in its documentation!\n",
    "\n",
    "In the future, you will also use `Adam`optimizer, but for the moment we are focusing on `SGD`. Note, however, that they all have (more or less) the same interface --- to create an optimizer instance, you need to pass the parameters to be updated and specify the learning rate (lr).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with our concrete problem: to create an optimizer we start by creating an object `SGD` with: \n",
    "\n",
    "- the parameters under consideration\n",
    "- the lr parameter, \n",
    "\n",
    "(see the code below). The parameters we want to learn are `w` and `w0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.randn(1,requires_grad=True)\n",
    "w0= th.randn(1,requires_grad=True)\n",
    "trainable_parameters = (w,w0)\n",
    "sgd = th.optim.SGD(trainable_parameters, lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Do the following:** \n",
    "\n",
    "- Explain the parameter `lr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two important functions you will use are:\n",
    "\n",
    "- `step()` --- This function updates all trainable parameters using their computed gradients (e.g., w.grad and w0.grad).\n",
    "\n",
    "- `zero_grad()` --- This function resets the gradient values, preparing them for the next training step.\n",
    "\n",
    "\n",
    "### **Do the following:** \n",
    "- Compute Predictions:\n",
    "        \n",
    "        Compute the predictions for the input data `X`.\n",
    "\n",
    "- Backward Propagation:\n",
    "    \n",
    "        Do the backpropagation and print the values of `w` and `w0` along with their gradients.\n",
    "\n",
    "- Update Parameters:\n",
    "   \n",
    "        Make an update using the optimizer and print the updated values of `w` and `w0` with their gradients.\n",
    "\n",
    "- Reset Gradients:\n",
    "    \n",
    "        Run `zero_grad()` to reset the gradients and print the values of `w` and `w0` with their gradients after resetting.\n",
    "\n",
    "- Plot the New Line:\n",
    "        \n",
    "        Plot the new line defined by the new values of `w` and `w0`.\n",
    "\n",
    "- Evaluate Learning Rate:\n",
    "    \n",
    "        Do you think value of the learning rate `lr` is adapted for this task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many loss functions are already implemented in PyTorch (see https://pytorch.org/docs/stable/nn.html#loss-functions for the full list). In our lab sessions we will focus on: \n",
    "\n",
    "- `MSELoss` for regression \n",
    "- `BCELoss` for binary classification\n",
    "- `CrossEntropyLoss` and `NLLLoss` for multiclass classification\n",
    "\n",
    "\n",
    "- Read the documentation of these losses and apply the `MSELoss` to the prediction for `X`\n",
    "- Repeat all the steps from the previous TODO list and include a calculation of the `MSELoss`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) The training function\n",
    "\n",
    "Now you have everything to write the training code of the model!\n",
    "\n",
    "\n",
    "### **Do the following:** \n",
    "\n",
    "- Initialize the model parameters before the training begins.\n",
    "\n",
    "- Perform SGD in a loop: iterate over multiple epochs, updating parameters using the optimizer.\n",
    "\n",
    "- Record how the loss evolves after each epoch.\n",
    "\n",
    "- Plot the loss evolution during the training process.\n",
    "\n",
    "- Plot the new decision boundary and observe how it changed during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 10\n",
    "lr = 1e-1\n",
    "## Your code here \n",
    "# def training(what do you need here): \n",
    "# .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Module\n",
    "\n",
    "In the previous section we coded our linear model \"by hand\". In practice, it is more convenient to use existing modules (base class `Module`). For instance the linear transform $f(x) = wx+w_0$ is simply the application of a so-called `Linear` module. \n",
    "\n",
    "### **Do the following:**\n",
    "\n",
    "- Look at the documentation\n",
    "\n",
    "- Rewrite the training code such that it uses a `Linear` module. \n",
    "\n",
    "- Look at the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Logistic Regression\n",
    "\n",
    "Let start with an easy dataset for binary classification. The following subsections just provide a synthetic dataset and a function to visualize it. \n",
    "\n",
    "\n",
    "\n",
    "### 8.1)  Create the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ST1 = np.array([[17.0 ,12 ,13 ,15 ,15 ,20 ,20],[ 10 ,12 ,14 ,15 ,20 ,15 ,20]]) # class 1 \n",
    "ST2 = np.array([4, 7.5, 10 ,11, 5 ,5 ,6, 8, 5, 0, 5, 0, 10, 6]).reshape(2,7) # class 2 \n",
    "Xstudents = np.concatenate((ST1,ST2),axis=1)\n",
    "Ystudents = np.ones(14)\n",
    "Ystudents[7:] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Do the following:** \n",
    "\n",
    "- Plot the dataset with two colors (one for each class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2) The model \n",
    "\n",
    "\n",
    "In this section, you will define, train, and visualize a logistic regression model using PyTorch.\n",
    "\n",
    "\n",
    "The roadmap is: \n",
    "\n",
    "- Convert Data to Tensors:\n",
    "\n",
    "        As explained PyTorch models utilises Tensor objects. Make sure that your dataset is converted into PyTorch tensors before training.\n",
    "\n",
    "- Create the Logistic Regression Model:\n",
    "\n",
    "        Use `torch.nn.Sequential` to define a simple neural model. The model should consist of a single neuron with a logistic activation function (i.e., a linear layer followed by a sigmoid function).\n",
    "\n",
    "- Define the Optimizer:\n",
    "\n",
    "        Take the basic Stochastic Gradient Descent (SGD) as the optimization algorithm.\n",
    "\n",
    "- Define the Objective (Loss) Function\n",
    "\n",
    "- Train the Model:\n",
    "\n",
    "        Implement a training loop that iterates until convergence. It can be useful to play with different learning rates to observe their effects. Perform gradient descent step-by-step, updating the model parameters after each example.\n",
    "\n",
    "- Look at the solution:\n",
    "\n",
    "        Visualize the decision boundary and check how well the model has learned.\n",
    "\n",
    "- Train in Batch Mode:\n",
    "\n",
    "        Start again in **batch** mode -- instead of updating the model after each example, compute gradients using the entire training set.\n",
    "\n",
    "\n",
    "### **Do all the things in the list above** \n",
    "\n",
    "#### From data to tensors / variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model, its loss and optimizer\n",
    "\n",
    "The model is a linear transformation followed by a Sigmoid function. This is equivalent to a logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model \n",
    "D_in=2  # input size : 2 \n",
    "D_out=1 # output size: one value \n",
    "\n",
    "# The loss\n",
    "\n",
    "# The optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model with data\n",
    "You can run inference to see if everything is fine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a single input vector \n",
    "prediction = model(X[0]) # or prediction = model.forward(X[0]) both are equivalent\n",
    "\n",
    "print(\"For the first input: \",prediction)\n",
    "\n",
    "# With 3 input vectors \n",
    "prediction = model(X[0:3])\n",
    "print(\"For the 3 first inputs: \",prediction)\n",
    "\n",
    "# For the whole dataset\n",
    "prediction = model(X)\n",
    "print(\"For all: \",prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a single input vector \n",
    "prediction = model(X[0])\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Y[0], Y[0].shape)\n",
    "\n",
    "loss_fn(prediction,Y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should generate a warning or an error, since the label (or target value) and the prediction (considered as the input value of the loss) are of different dimensions. \n",
    "\n",
    "There is two ways to fix this. The first one is to reduce the input dimension using `squeeze`. The second one is to modify the target values. See the two cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X[0]).squeeze()\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Y[0], Y[0].shape)\n",
    "\n",
    "loss_fn(prediction,Y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X[0])\n",
    "Ymodified = Y.view(-1,1)\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Ymodified[0], Ymodified[0].shape)\n",
    "\n",
    "loss_fn(prediction,Ymodified[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to visualize the evolution of the loss function: to be sure that everything went well. The idea is to store the loss values in a numpy array and then to plot it. \n",
    "\n",
    "####  Plot the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the \"solution\" \n",
    "\n",
    "Here, we look at the different wrapping steps: \n",
    "- The model is a set of modules\n",
    "- A Linear module is a matrix of weights along with a bias vector. They are parameters.\n",
    "- A Parameter wrap a tensor\n",
    "- A tensor can be casted as a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model[0]\n",
    "print(type(mod))\n",
    "print(type(mod.bias))\n",
    "print(type(mod.bias.data))\n",
    "print(type(mod.bias.data.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod.bias.data.view(1,1))\n",
    "print(mod.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Impact of the learning rate \n",
    "\n",
    "Now, we will use the same model, but trained with a different learning rate. The training process restarts from scratch. We need to therefore re-create the model and the associated optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = th.nn.Sequential(\n",
    "    th.nn.Linear(D_in, D_out),\n",
    "    th.nn.Sigmoid()    \n",
    ")\n",
    "learning_rate = 1e-1\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model, with randomly initialized parameters, we can train it using a different learning rate --- a larger one. \n",
    "\n",
    "#### **Do the following:**\n",
    "\n",
    "- Run the training with the same number of epochs and compare the loss value we get at the end\n",
    "- Do you think we can reach the same value with the learning rate of 1e-2, but with a longer training ? \n",
    "- Try the same thing with a learning rate of 0.5, what do you observe ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "351px",
    "width": "423px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
